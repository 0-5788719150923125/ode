{"mappings":";;;;;;;AEGA,SAAS,mCACL,GAAG,EACH,QAAQ,gEAAgE;IAExE,IAAI,OAAO;IACX,IAAK,IAAI,IAAI,GAAG,IAAI,KAAK,IACrB,QAAQ,MAAM,MAAM,CAAC,KAAK,KAAK,CAAC,KAAK,MAAM,KAAK,MAAM,MAAM;IAEhE,OAAO;AACX;AAEA,UAAU;IACN,IAAI,IAAI;IACR,MAAO,IAAI,IAAK;QACZ;QACA,MAAM,mCAAa;IACvB;AACJ;AAEO,eAAe,0CAClB,SAAS;AACT,gBAAgB,qCAAe;IAG/B,SAAS;IACT,MAAM,YAAY,GAAG,6DAA6D;;IAClF,MAAM,aAAa,EAAE,8DAA8D;;IACnF,MAAM,SAAS,IAAI,2DAA2D;;IAC9E,MAAM,mBAAmB,MAAM,8DAA8D;;IAC7F,MAAM,YAAY,IAAI,+DAA+D;;IACrF,MAAM,kBAAkB,OAAO,qEAAqE;;IACpG,MAAM,gBAAgB,IAAI,0DAA0D;;IACpF,MAAM,eAAe;QAAC;QAAG;QAAM;QAAK;QAAM;KAAE;IAE5C,MAAM,OAAO;IACb,iCAAiC;IAEjC,2CAA2C;IAC3C,yCAAyC;IACzC,qBAAqB;IACrB,UAAU;IACV,OAAO;IAEP,IAAI,OAAO,KAAK,IAAI,GAAG,KAAK;IAE5B,8DAA8D;IAC9D,MAAM,UAAU,MAAM,IAAI,CAAC,IAAI,IAAI,MAAM,IAAI,CAAC;IAC9C,MAAM,EAAE,QAAQ,WAAW,EAAE,GAAG;IAEhC,oFAAoF;IACpF,mFAAmF;IACnF,4BAA4B;IAE5B,qFAAqF;IACrF,sEAAsE;IACtE,MAAM,aAAa,KAAK,KAAK,CAAC,KAAK,MAAM,KAAM,CAAA,KAAK,MAAM,GAAG,YAAY,CAAA;IAEzE,uEAAuE;IACvE,MAAM,OAAO,KAAK,KAAK,CAAC,YAAY,aAAa;IAEjD,MAAM,cAAc,IAAI,YACpB,MAAM,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,IAAM,QAAQ,OAAO,CAAC;IAGhD,IAAK,IAAI,IAAI,GAAG,IAAI,QAAQ,EAAE,EAAG;QAC7B,MAAM,CAAC,IAAI,GAAG,GAAG,mCACb,KAAK,MAAM,EACX,WACA,YACA,aACA,aACA;QAGJ,2DAA2D;QAC3D,6BAA6B;QAC7B,MAAM,IAAI,CAAC,KAAK,CAAC,GAAG,CAAC,IAAI,IAAI;YACzB,QAAQ;uBACR;6BACA;YACA,WAAW;gBACP,cAAc;oBACV,QAAQ,GAAG,CAAC,CAAC,MAAM,EAAE,IAAI,EAAE,IAAI,EAAE,OAAO,CAAC,CAAC;gBAC9C;gBACA,YAAY,CAAC,OAAO;oBAChB,QAAQ,GAAG,CACP,CAAC,MAAM,EAAE,QAAQ,EAAE,kBAAkB,EAAE,KAAK,IAAI,CAAC,QAAQ,EAAE,CAAC,EAAE,CAAC,CAAC;gBAExE;gBACA,YAAY,OAAO,OAAO;oBACtB,wCAAwC;oBACxC,QAAQ,GAAG,CAAC;oBACZ,QAAQ,GAAG,CAAC,MAAM,IAAI,CAAC,QAAQ,CAAC,MAAM;gBACtC,eAAe;gBACf,mEAAmE;gBACnE,IAAI;gBACR;YAaJ;QACJ;QAEA,GAAG,OAAO;QACV,GAAG,OAAO;IACd;AACJ;AAEA,MAAM,qCAAe,CACjB,YACA,WACA,YACA,aACA,aACA;IAEA,MAAM,kBAAkB,EAAE;IAE1B,IAAK,IAAI,IAAI,GAAG,IAAI,aAAa,YAAY,GAAG,KAAK,WACjD,gBAAgB,IAAI,CAAC;IAGzB,YAAQ,OAAO,CAAC;IAEhB,MAAM,WAAW,IAAI,oBAAgB;QAAC;QAAa;QAAW;KAAY;IAE1E,MAAM,WAAW,IAAI,oBAAgB;QAAC;QAAa;KAAY;IAE/D,IAAK,IAAI,IAAI,GAAG,IAAI,aAAa,EAAE,EAAG;QAClC,MAAM,aAAa,eAAe,CAAC,IAAI,gBAAgB,MAAM,CAAC;QAC9D,IAAK,IAAI,IAAI,GAAG,IAAI,WAAW,EAAE,EAC7B,SAAS,GAAG,CAAC,GAAG,GAAG,GAAG,WAAW,CAAC,aAAa,EAAE;QAErD,SAAS,GAAG,CAAC,GAAG,GAAG,WAAW,CAAC,aAAa,UAAU;IAC1D;IAEA,OAAO;QAAC,SAAS,QAAQ;QAAI,SAAS,QAAQ;KAAG;AACrD;;;ADjJe;IACX,YACI,aAAa,EACb,SAAS,EACT,WAAW;IACX,eAAe;IACf,YAAY,EACZ,aAAa,CACf;QACE,IAAI,CAAC,aAAa,GAAG;QACrB,IAAI,CAAC,SAAS,GAAG;QACjB,IAAI,CAAC,OAAO,GAAG,MAAM,IAAI,CAAC,IAAI,IAAI,MAAM,IAAI,CAAC;QAC7C,IAAI,CAAC,WAAW,GAAG;QACnB,kBAAkB;QAClB,qCAAqC;QACrC,yBAAyB;QACzB,iCAAiC;QACjC,IAAI,CAAC,YAAY,GAAG;QACpB,IAAI,CAAC,aAAa,GAAG;QACrB,IAAI,CAAC,KAAK,GAAG;QACb,IAAI,CAAC,IAAI;IACb;IAEA,OAAO;QACH,sFAAsF;QACtF,yFAAyF;QACzF,wBAAwB;QACxB,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,CAClC,CAAC,KAAK,eAAe,GAAG;YACpB,IAAI,GAAG,CACH,cAAU,IAAI,CAAC;gBACX,OAAO;gBACP,8EAA8E;gBAC9E,+EAA+E;gBAC/E,uBAAuB;gBACvB,iBAAiB,IAAI,KAAK,MAAM,GAAG;gBACnC,gFAAgF;gBAChF,mFAAmF;gBACnF,mFAAmF;gBACnF,mFAAmF;gBACnF,mFAAmF;gBACnF,mFAAmF;gBACnF,eAAe;gBACf,YACI,MAAM,IACA;oBAAC,IAAI,CAAC,SAAS;oBAAE,IAAI,CAAC,WAAW;iBAAC,GAClC;YACd;YAEJ,yFAAyF;YACzF,sEAAsE;YACtE,OAAO;QACX,GACA;QAGJ,wFAAwF;QACxF,uFAAuF;QACvF,qFAAqF;QACrF,8FAA8F;QAC9F,wCAAwC;QACxC,IAAI,CAAC,KAAK,CAAC,GAAG,CACV,cAAU,KAAK,CAAC;YACZ,OAAO,IAAI,CAAC,WAAW;YACvB,YAAY;QAChB;QAGJ,uFAAuF;QACvF,uFAAuF;QACvF,iFAAiF;QACjF,wFAAwF;QACxF,0BAA0B;QAC1B,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC;YACf,WAAW,aAAS,OAAO,CAAC,IAAI,CAAC,YAAY;YAC7C,MAAM;QACV;IACJ;IAEA,WAAW;QACP,OAAO,IAAI,CAAC,KAAK;IACrB;IAEA,MAAM,WAAW,aAAa,EAAE;QAC5B,MAAM,QAAQ,CAAA,GAAA,yCAAS,EAAE,IAAI,CAAC,IAAI;QAClC,MAAM,MAAM;IAChB;IAEA,aAAa;QACT,OAAO,IAAI,CAAC,KAAK,CAAC,UAAU;IAChC;IAEA,MAAM,SAAS,IAAI,EAAE,cAAc,GAAG,EAAE;QACpC,OAAO,MAAM,+BACT,IAAI,CAAC,KAAK,EACV,MACA,IAAI,CAAC,SAAS,EACd,IAAI,CAAC,WAAW,EAChB,IAAI,CAAC,OAAO,EACZ,IAAI,CAAC,aAAa,EAClB;IAER;AACJ;AAEA,eAAe,+BACX,KAAK,EACL,IAAI,EACJ,SAAS,EACT,WAAW,EACX,OAAO,EACP,aAAa,EACb,WAAW;IAEX,oEAAoE;IACpE,iBAAiB;IACjB,IAAI,kBAAkB,MAAM,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,IAAM,QAAQ,OAAO,CAAC;IAElE,IAAI,YAAY;IAEhB,sEAAsE;IACtE,0EAA0E;IAC1E,0EAA0E;IAC1E,yEAAyE;IACzE,0BAA0B;IAC1B,MAAO,UAAU,MAAM,GAAG,cAAe;QACrC,MAAM,cAAc,IAAI,oBAAgB;YAAC;YAAG;YAAW;SAAY;QAElE;eAAI,MAAM;SAAW,CAAC,GAAG,CAAC,CAAC,GAAG,IAC3B,YAAY,GAAG,CAAC,GAAG,GAAG,GAAG,eAAe,CAAC,EAAE;QAG/C,MAAM,QAAQ,YAAY,QAAQ;QAClC,MAAM,SAAS,MAAM,OAAO,CAAC;QAE7B,kEAAkE;QAClE,uEAAuE;QACvE,yCAAyC;QACzC,MAAM,CAAC,YAAY,GAAG,YAAQ,IAC1B,8EAA8E;YAC9E,sCAAsC;YACtC,gFAAgF;YAChF,sDAAsD;YACtD,mBAEQ,iEAAiE;YACjE,WACI,WAAO,eAAW,UAClB,KAAK,GAAG,CAAC,aAAa,QAE1B,GACA,MACA,OAEH,QAAQ;QAGjB,yEAAyE;QACzE,6CAA6C;QAC7C,MAAM,OAAO;QACb,OAAO,OAAO;QAEd,2EAA2E;QAC3E,2EAA2E;QAC3E,gFAAgF;QAChF,4EAA4E;QAC5E,aAAa,OAAO,CAAC,YAAY;QACjC,kBAAkB,gBAAgB,KAAK,CAAC;QACxC,gBAAgB,IAAI,CAAC;IACzB;IACA,+EAA+E;IAC/E,OAAO;AACX;;;ID/KA,2CAAe,CAAA,GAAA,wCAAI","sources":["src/index.js","src/model.js","src/train.js"],"sourcesContent":["import Model from './model'\nexport default Model\n","import '@tensorflow/tfjs-node'\nimport * as tf from '@tensorflow/tfjs'\nimport { trainModel } from './train'\n\nexport default class ModelPrototype {\n    constructor(\n        lstmLayerSize,\n        sampleLen,\n        // charSet,\n        // charSetSize,\n        learningRate,\n        displayLength\n    ) {\n        this.lstmLayerSize = lstmLayerSize\n        this.sampleLen = sampleLen\n        this.charSet = Array.from(new Set(Array.from('this is training data')))\n        this.charSetSize = 62\n        // const charSet =\n        // const charSetSize = charSet.length\n        // this.charSet = charSet\n        // this.charSetSize = charSetSize\n        this.learningRate = learningRate\n        this.displayLength = displayLength\n        this.model = null\n        this.init()\n    }\n\n    init() {\n        // XXX: Create our processing model. We iterate through the array of lstmLayerSize and\n        //      iteratively add an LSTM processing layer whose number of internal units match the\n        //      specified value.\n        this.model = this.lstmLayerSize.reduce(\n            (mdl, lstmLayerSize, i, orig) => {\n                mdl.add(\n                    tf.layers.lstm({\n                        units: lstmLayerSize,\n                        // XXX: For all layers except the last one, we specify that we'll be returning\n                        //      sequences of data. This allows us to iteratively chain individual LSTMs\n                        //      to one-another.\n                        returnSequences: i < orig.length - 1,\n                        // XXX: Since each LSTM layer generates a sequence of data, only the first layer\n                        //      needs to receive a specific input shape. Here, we initialize the inputShape\n                        //      [sampleLen, charSetSize]. This defines that the first layer will receive an\n                        //      input matrix which allows us to convert from our selected sample range into\n                        //      the size of our charset. The charset uses one-hot encoding, which allows us\n                        //      to represent each possible character in our dataset using a dedicated input\n                        //      neuron.\n                        inputShape:\n                            i === 0\n                                ? [this.sampleLen, this.charSetSize]\n                                : undefined\n                    })\n                )\n                // XXX: Here we use a sequential processing model for our network. This model gets passed\n                //      between each iteration, and is what we add our LSTM layers to.\n                return mdl\n            },\n            tf.sequential()\n        )\n\n        // XXX: At the output, we use a softmax function (a normalized exponential) as the final\n        //      classification layer. This is common in many neural networks. It's particularly\n        //      important for this example, because we use the logit probability model (which\n        //      supports regression for networks with more than 2 possible outcomes of a categorically\n        //      distributed dependent variable).\n        this.model.add(\n            tf.layers.dense({\n                units: this.charSetSize,\n                activation: 'softmax'\n            })\n        )\n\n        // XXX: Finally, compile the model. The optimizer is used to define the backpropagation\n        //      technique that should be used for training. We use the rmsProp to help tune the\n        //      learning rate that we apply individually to each neuron to help learning.\n        //      We use a categoricalCrossentropy loss model which is compatible with our softmax\n        //      activation output.\n        this.model.compile({\n            optimizer: tf.train.rmsprop(this.learningRate),\n            loss: 'categoricalCrossentropy'\n        })\n    }\n\n    getModel() {\n        return this.model\n    }\n\n    async trainModel(dataGenerator) {\n        const bound = trainModel.bind(this)\n        await bound(dataGenerator)\n    }\n\n    getWeights() {\n        return this.model.getWeights()\n    }\n\n    async generate(seed, temperature = 0.7) {\n        return await generate(\n            this.model,\n            seed,\n            this.sampleLen,\n            this.charSetSize,\n            this.charSet,\n            this.displayLength,\n            temperature\n        )\n    }\n}\n\nasync function generate(\n    model,\n    seed,\n    sampleLen,\n    charSetSize,\n    charSet,\n    displayLength,\n    temperature\n) {\n    // XXX: Fetch the sequence of numeric values which correspond to the\n    //      sentence.\n    let sentenceIndices = Array.from(seed).map((e) => charSet.indexOf(e))\n\n    let generated = ''\n\n    // XXX: Note that since the displayLength is arbitrary, we can make it\n    //      much larger than our sampleLen. This loop will continue to iterate\n    //      about the sentenceIndices and buffering the output of the network,\n    //      which permits it to continue generating far past our initial seed\n    //      has been provided.\n    while (generated.length < displayLength) {\n        const inputBuffer = new tf.TensorBuffer([1, sampleLen, charSetSize])\n\n        ;[...Array(sampleLen)].map((_, i) =>\n            inputBuffer.set(1, 0, i, sentenceIndices[i])\n        )\n\n        const input = inputBuffer.toTensor()\n        const output = model.predict(input)\n\n        // XXX: Pick the character the RNN has decided is the most likely.\n        //      tf.tidy cleans all of the allocated tensors within the function\n        //      scope after it has been executed.\n        const [winnerIndex] = tf.tidy(() =>\n            // XXX: Draws samples from a multinomial distribution (these are distributions\n            //      involving multiple variables).\n            //      tf.squeeze remove dimensions of size (1) from the supplied tensor. These\n            //      are then divided by the specified temperature.\n            tf\n                .multinomial(\n                    // XXX: Use the temperature to control the network's spontaneity.\n                    tf.div(\n                        tf.log(tf.squeeze(output)),\n                        Math.max(temperature, 1e-6)\n                    ),\n                    1,\n                    null,\n                    false\n                )\n                .dataSync()\n        )\n\n        // XXX: Always clean up tensors once you're finished with them to improve\n        //      memory utilization and prevent leaks.\n        input.dispose()\n        output.dispose()\n\n        // XXX: Here we append the generated character to the resulting string, and\n        //      add this char to the sliding window along the sentenceIndices. This\n        //      is how we continually wrap around the same buffer and generate arbitrary\n        //      sequences of data even though our network only accepts fixed inputs.\n        generated += charSet[winnerIndex]\n        sentenceIndices = sentenceIndices.slice(1)\n        sentenceIndices.push(winnerIndex)\n    }\n    // console.log(`Generated text (temperature=${temperature}):\\n ${generated}\\n`)\n    return generated\n}\n","import '@tensorflow/tfjs-node'\nimport * as tf from '@tensorflow/tfjs'\n\nfunction randomString(\n    len,\n    chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n) {\n    let text = ''\n    for (let i = 0; i < len; i++) {\n        text += chars.charAt(Math.floor(Math.random() * chars.length))\n    }\n    return text\n}\n\nfunction* infiniteNumbers() {\n    let i = 0\n    while (i < 500) {\n        i++\n        yield randomString(10000)\n    }\n}\n\nexport async function trainModel(\n    // model,\n    dataGenerator = infiniteNumbers\n    // generateFunction\n) {\n    // XXX: .\n    const sampleLen = 60 // length of a sequence of characters we'll pass into the RNN\n    const sampleStep = 3 // number of characters to jump between segments of input text\n    const epochs = 150 // the total number of times to update the training weights\n    const examplesPerEpoch = 10000 // the number of text segments to train against for each epoch\n    const batchSize = 128 // hyperparameter controlling the frequency weights are updated\n    const validationSplit = 0.0625 // fraction of training data which will be treated as validation data\n    const displayLength = 120 // how many characters you want to generate after training\n    const temperatures = [0, 0.25, 0.5, 0.75, 1]\n\n    const data = dataGenerator()\n    // console.log(data.next().value)\n\n    // XXX: Fetch the text data to sample from.\n    //   const { data: text } = await axios({\n    //     method: 'get',\n    //     url\n    //   })\n\n    let text = data.next().value\n\n    // XXX: Fetch all unique characters in the dataset. (quickly!)\n    const charSet = Array.from(new Set(Array.from(text)))\n    const { length: charSetSize } = charSet\n\n    // XXX: Convert the total input character text into the corresponding indices in the\n    //      charSet. This is how we map consistently between character data and numeric\n    //      neural network dataj\n\n    // XXX: Pick a random position to start in the dataset. (Note that we choose an index\n    //      which cannot exceed the minimum size of our sampleLength - 1).\n    const startIndex = Math.round(Math.random() * (text.length - sampleLen - 1))\n\n    // XXX: Create the seed data which we'll use to initialize the network.\n    const seed = text.slice(startIndex, startIndex + sampleLen)\n\n    const textIndices = new Uint16Array(\n        Array.from(text).map((e) => charSet.indexOf(e))\n    )\n\n    for (let i = 0; i < epochs; ++i) {\n        const [xs, ys] = dataToTensor(\n            text.length,\n            sampleLen,\n            sampleStep,\n            charSetSize,\n            textIndices,\n            examplesPerEpoch\n        )\n\n        // XXX: Fit the model and hold up iteration of the for loop\n        //      until it is finished.\n        await this.model.fit(xs, ys, {\n            epochs: 1,\n            batchSize,\n            validationSplit,\n            callbacks: {\n                onTrainBegin: () => {\n                    console.log(`Epoch ${i + 1} of ${epochs}:`)\n                },\n                onEpochEnd: (epoch, logs) => {\n                    console.log(\n                        `Epoch ${epoch + 1} completed. Loss: ${logs.loss.dataSync()[0]}`\n                    )\n                },\n                onBatchEnd: async (batch, logs) => {\n                    // Access batch number and training logs\n                    console.log(logs)\n                    console.log(await this.generate(seed, 0.7))\n                    // console.log(\n                    //     `Batch ${batch} completed. Loss: ${logs.loss.dataSync()[0]}`\n                    // )\n                }\n                // onTrainEnd: () =>\n                //   temperatures.map((temp) =>\n                //     generate(\n                //       model,\n                //       seed,\n                //       sampleLen,\n                //       charSetSize,\n                //       charSet,\n                //       displayLength,\n                //       temp\n                //     )\n                //   )\n            }\n        })\n\n        xs.dispose()\n        ys.dispose()\n    }\n}\n\nconst dataToTensor = (\n    textLength,\n    sampleLen,\n    sampleStep,\n    charSetSize,\n    textIndices,\n    numExamples\n) => {\n    const trainingIndices = []\n\n    for (let i = 0; i < textLength - sampleLen - 1; i += sampleStep) {\n        trainingIndices.push(i)\n    }\n\n    tf.util.shuffle(trainingIndices)\n\n    const xsBuffer = new tf.TensorBuffer([numExamples, sampleLen, charSetSize])\n\n    const ysBuffer = new tf.TensorBuffer([numExamples, charSetSize])\n\n    for (let i = 0; i < numExamples; ++i) {\n        const beginIndex = trainingIndices[i % trainingIndices.length]\n        for (let j = 0; j < sampleLen; ++j) {\n            xsBuffer.set(1, i, j, textIndices[beginIndex + j])\n        }\n        ysBuffer.set(1, i, textIndices[beginIndex + sampleLen])\n    }\n\n    return [xsBuffer.toTensor(), ysBuffer.toTensor()]\n}\n"],"names":[],"version":3,"file":"index.js.map","sourceRoot":"../"}