{"mappings":";;;;;ACGO,eAAe,0CACpB,aAAa,EACb,SAAS,EACT,WAAW,EACX,YAAY;IAEZ,sFAAsF;IACtF,yFAAyF;IACzF,wBAAwB;IACxB,MAAM,QAAQ,cAAc,MAAM,CAAC,CAAC,KAAK,eAAe,GAAG;QACzD,QAAQ,GAAG,CAAC;QACZ,IAAI,GAAG,CACL,cAAU,IAAI,CAAC;YACb,OAAO;YACP,8EAA8E;YAC9E,+EAA+E;YAC/E,uBAAuB;YACvB,iBAAiB,IAAI,KAAK,MAAM,GAAG;YACnC,gFAAgF;YAChF,mFAAmF;YACnF,mFAAmF;YACnF,mFAAmF;YACnF,mFAAmF;YACnF,mFAAmF;YACnF,eAAe;YACf,YAAY,MAAM,IAAI;gBAAC;gBAAW;aAAY,GAAG;QACnD;QAEF,yFAAyF;QACzF,sEAAsE;QACtE,OAAO;IACT,GAAG;IAEH,wFAAwF;IACxF,uFAAuF;IACvF,qFAAqF;IACrF,8FAA8F;IAC9F,wCAAwC;IACxC,MAAM,GAAG,CACP,cAAU,KAAK,CAAC;QACd,OAAO;QACP,YAAY;IACd;IAGF,uFAAuF;IACvF,uFAAuF;IACvF,iFAAiF;IACjF,wFAAwF;IACxF,0BAA0B;IAC1B,MAAM,OAAO,CAAC;QACZ,WAAW,aAAS,OAAO,CAAC;QAC5B,MAAM;IACR;IAEA,OAAO;AACT","sources":["src/index.js","src/models.js"],"sourcesContent":["import { createModel } from './models'\n","import '@tensorflow/tfjs-node'\nimport * as tf from '@tensorflow/tfjs'\n\nexport async function createModel(\n  lstmLayerSize,\n  sampleLen,\n  charSetSize,\n  learningRate\n) {\n  // XXX: Create our processing model. We iterate through the array of lstmLayerSize and\n  //      iteratively add an LSTM processing layer whose number of internal units match the\n  //      specified value.\n  const model = lstmLayerSize.reduce((mdl, lstmLayerSize, i, orig) => {\n    console.log(i)\n    mdl.add(\n      tf.layers.lstm({\n        units: lstmLayerSize,\n        // XXX: For all layers except the last one, we specify that we'll be returning\n        //      sequences of data. This allows us to iteratively chain individual LSTMs\n        //      to one-another.\n        returnSequences: i < orig.length - 1,\n        // XXX: Since each LSTM layer generates a sequence of data, only the first layer\n        //      needs to receive a specific input shape. Here, we initialize the inputShape\n        //      [sampleLen, charSetSize]. This defines that the first layer will receive an\n        //      input matrix which allows us to convert from our selected sample range into\n        //      the size of our charset. The charset uses one-hot encoding, which allows us\n        //      to represent each possible character in our dataset using a dedicated input\n        //      neuron.\n        inputShape: i === 0 ? [sampleLen, charSetSize] : undefined\n      })\n    )\n    // XXX: Here we use a sequential processing model for our network. This model gets passed\n    //      between each iteration, and is what we add our LSTM layers to.\n    return mdl\n  }, tf.sequential())\n\n  // XXX: At the output, we use a softmax function (a normalized exponential) as the final\n  //      classification layer. This is common in many neural networks. It's particularly\n  //      important for this example, because we use the logit probability model (which\n  //      supports regression for networks with more than 2 possible outcomes of a categorically\n  //      distributed dependent variable).\n  model.add(\n    tf.layers.dense({\n      units: charSetSize,\n      activation: 'softmax'\n    })\n  )\n\n  // XXX: Finally, compile the model. The optimizer is used to define the backpropagation\n  //      technique that should be used for training. We use the rmsProp to help tune the\n  //      learning rate that we apply individually to each neuron to help learning.\n  //      We use a categoricalCrossentropy loss model which is compatible with our softmax\n  //      activation output.\n  model.compile({\n    optimizer: tf.train.rmsprop(learningRate),\n    loss: 'categoricalCrossentropy'\n  })\n\n  return model\n}\n"],"names":[],"version":3,"file":"index.js.map","sourceRoot":"../"}