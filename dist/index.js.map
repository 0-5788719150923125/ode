{"mappings":";;;;;;;AEGO,eAAe,0CAAW,aAAa;IAC1C,SAAS;IACT,MAAM,YAAY,GAAG,6DAA6D;;IAClF,MAAM,aAAa,EAAE,8DAA8D;;IACnF,MAAM,SAAS,IAAI,2DAA2D;;IAC9E,MAAM,mBAAmB,MAAM,8DAA8D;;IAC7F,MAAM,YAAY,IAAI,+DAA+D;;IACrF,MAAM,kBAAkB,OAAO,qEAAqE;;IAEpG,MAAM,OAAO;IACb,IAAK,IAAI,IAAI,GAAG,IAAI,QAAQ,EAAE,EAAG;QAC7B,MAAM,KAAK,YAAQ,SAAS,CACxB,2CAAqB,eAAe,KAAK,UAAU;QAEvD,MAAM,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,IAAI;YAC5B,QAAQ;uBACR;YACA,mBAAmB;YACnB,WAAW;gBACP,cAAc,IAAM,QAAQ,GAAG,CAAC,CAAC,MAAM,EAAE,IAAI,EAAE,IAAI,EAAE,OAAO,CAAC,CAAC;gBAC9D,YAAY,OAAO,OAAO,OAAS,QAAQ,GAAG,CAAC;gBAC/C,YAAY,OAAO,OAAO,OACtB,QAAQ,GAAG,CAAC,MAAM,IAAI,CAAC,QAAQ,CAAC,MAAM;YAC9C;QACJ;IACJ;AACJ;AAEA,SAAS,2CAAqB,aAAa;IACvC,OAAO;QACH,OAAO,qCAAe,eAAe;IACzC;AACJ;AAEA,UAAU,qCAAe,aAAa,EAAE,WAAU;IAC9C,QAAQ,GAAG,CAAC;IACZ,MAAO,KAAM;QACT,MAAM,OAAO,cAAc,IAAI,GAAG,KAAK;QAEvC,QAAQ,GAAG,CAAC;QAEZ,oDAAoD;QACpD,MAAM,aAAa,KAAK,MAAM;QAC9B,MAAM,YAAY,GAAG,mBAAmB;;QACxC,MAAM,aAAa,EAAE,mBAAmB;;QACxC,wDAAwD;QACxD,qCAAqC;QAErC,uCAAuC;QACvC,MAAM,cAAc,IAAI,YACpB,MAAM,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,IAAM,YAAW,OAAO,CAAC;QAEnD,MAAM,kBAAkB,EAAE;QAE1B,IAAK,IAAI,IAAI,GAAG,IAAI,aAAa,YAAY,GAAG,KAAK,WACjD,gBAAgB,IAAI,CAAC;QAGzB,YAAQ,OAAO,CAAC;QAEhB,MAAM,WAAW,IAAI,oBAAgB;YAAC;YAAG;YAAW,YAAW,MAAM;SAAC,EAAE,wBAAwB;;QAChG,MAAM,WAAW,IAAI,oBAAgB;YAAC;YAAG,YAAW,MAAM;SAAC;QAE3D,MAAM,aAAa,eAAe,CAAC,IAAI,gBAAgB,MAAM,CAAC;QAC9D,IAAK,IAAI,IAAI,GAAG,IAAI,WAAW,EAAE,EAC7B,SAAS,GAAG,CAAC,GAAG,GAAG,GAAG,WAAW,CAAC,aAAa,EAAE;QAErD,SAAS,GAAG,CAAC,GAAG,GAAG,WAAW,CAAC,aAAa,UAAU;QAEtD,MAAM;YAAE,IAAI,SAAS,QAAQ;YAAI,IAAI,SAAS,QAAQ;QAAG;IAC7D;AACJ;;;ADtEe;IACX,YAAY,aAAa,EAAE,SAAS,EAAE,YAAY,EAAE,aAAa,CAAE;QAC/D,IAAI,CAAC,aAAa,GAAG;QACrB,IAAI,CAAC,SAAS,GAAG;QACjB,IAAI,CAAC,UAAU,GAAG,MAAM,IAAI,CACxB,IAAI,IACA,MAAM,IAAI,CACN,CAAC,gFAA6E,CAAC;QAI3F,IAAI,CAAC,YAAY,GAAG;QACpB,IAAI,CAAC,aAAa,GAAG;QACrB,IAAI,CAAC,KAAK,GAAG;QACb,IAAI,CAAC,IAAI;IACb;IAEA,OAAO;QACH,sFAAsF;QACtF,yFAAyF;QACzF,wBAAwB;QACxB,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,CAClC,CAAC,KAAK,eAAe,GAAG;YACpB,IAAI,GAAG,CACH,cAAU,IAAI,CAAC;gBACX,OAAO;gBACP,8EAA8E;gBAC9E,+EAA+E;gBAC/E,uBAAuB;gBACvB,iBAAiB,IAAI,KAAK,MAAM,GAAG;gBACnC,gFAAgF;gBAChF,mFAAmF;gBACnF,8FAA8F;gBAC9F,mFAAmF;gBACnF,mFAAmF;gBACnF,mFAAmF;gBACnF,eAAe;gBACf,YACI,MAAM,IACA;oBAAC,IAAI,CAAC,SAAS;oBAAE,IAAI,CAAC,UAAU,CAAC,MAAM;iBAAC,GACxC;YACd;YAEJ,yFAAyF;YACzF,sEAAsE;YACtE,OAAO;QACX,GACA;QAGJ,wFAAwF;QACxF,uFAAuF;QACvF,qFAAqF;QACrF,8FAA8F;QAC9F,wCAAwC;QACxC,IAAI,CAAC,KAAK,CAAC,GAAG,CACV,cAAU,KAAK,CAAC;YACZ,OAAO,IAAI,CAAC,UAAU,CAAC,MAAM;YAC7B,YAAY;QAChB;QAGJ,uFAAuF;QACvF,uFAAuF;QACvF,iFAAiF;QACjF,wFAAwF;QACxF,0BAA0B;QAC1B,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC;YACf,WAAW,aAAS,OAAO,CAAC,IAAI,CAAC,YAAY;YAC7C,MAAM;QACV;IACJ;IAEA,WAAW;QACP,OAAO,IAAI,CAAC,KAAK;IACrB;IAEA,MAAM,WAAW,aAAa,EAAE;QAC5B,MAAM,QAAQ,CAAA,GAAA,yCAAS,EAAE,IAAI,CAAC,IAAI;QAClC,MAAM,MAAM;IAChB;IAEA,aAAa;QACT,OAAO,IAAI,CAAC,KAAK,CAAC,UAAU;IAChC;IAEA,MAAM,SAAS,IAAI,EAAE,cAAc,GAAG,EAAE;QACpC,MAAM,QAAQ,+BAAS,IAAI,CAAC,IAAI;QAChC,OAAO,MAAM,MAAM,MAAM;IAC7B;AACJ;AAEA,eAAe,+BAAS,IAAI,EAAE,WAAW;IACrC,oEAAoE;IACpE,iBAAiB;IACjB,IAAI,kBAAkB,MAAM,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,IACxC,IAAI,CAAC,UAAU,CAAC,OAAO,CAAC;IAG5B,IAAI,YAAY;IAEhB,sEAAsE;IACtE,0EAA0E;IAC1E,0EAA0E;IAC1E,yEAAyE;IACzE,0BAA0B;IAC1B,MAAO,UAAU,MAAM,GAAG,IAAI,CAAC,aAAa,CAAE;QAC1C,MAAM,cAAc,IAAI,oBAAgB;YACpC;YACA,IAAI,CAAC,SAAS;YACd,IAAI,CAAC,UAAU,CAAC,MAAM;SACzB;QAEA;eAAI,MAAM,IAAI,CAAC,SAAS;SAAE,CAAC,GAAG,CAAC,CAAC,GAAG,IAChC,YAAY,GAAG,CAAC,GAAG,GAAG,GAAG,eAAe,CAAC,EAAE;QAG/C,MAAM,QAAQ,YAAY,QAAQ;QAClC,MAAM,SAAS,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC;QAElC,kEAAkE;QAClE,uEAAuE;QACvE,yCAAyC;QACzC,MAAM,CAAC,YAAY,GAAG,YAAQ,IAC1B,8EAA8E;YAC9E,sCAAsC;YACtC,gFAAgF;YAChF,sDAAsD;YACtD,mBAEQ,iEAAiE;YACjE,WACI,WAAO,eAAW,UAClB,KAAK,GAAG,CAAC,aAAa,QAE1B,GACA,MACA,OAEH,QAAQ;QAGjB,yEAAyE;QACzE,6CAA6C;QAC7C,MAAM,OAAO;QACb,OAAO,OAAO;QAEd,2EAA2E;QAC3E,2EAA2E;QAC3E,gFAAgF;QAChF,4EAA4E;QAC5E,aAAa,IAAI,CAAC,UAAU,CAAC,YAAY;QACzC,kBAAkB,gBAAgB,KAAK,CAAC;QACxC,gBAAgB,IAAI,CAAC;IACzB;IACA,+EAA+E;IAC/E,OAAO;AACX;;;IDhKA,2CAAe,CAAA,GAAA,wCAAI","sources":["src/index.js","src/model.js","src/train.js"],"sourcesContent":["import Model from './model'\nexport default Model\n","import '@tensorflow/tfjs-node'\nimport * as tf from '@tensorflow/tfjs'\nimport { trainModel } from './train'\n\nexport default class ModelPrototype {\n    constructor(lstmLayerSize, sampleLen, learningRate, displayLength) {\n        this.lstmLayerSize = lstmLayerSize\n        this.sampleLen = sampleLen\n        this.characters = Array.from(\n            new Set(\n                Array.from(\n                    `Â¶0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.?!'\"(){}[]| `\n                )\n            )\n        )\n        this.learningRate = learningRate\n        this.displayLength = displayLength\n        this.model = null\n        this.init()\n    }\n\n    init() {\n        // XXX: Create our processing model. We iterate through the array of lstmLayerSize and\n        //      iteratively add an LSTM processing layer whose number of internal units match the\n        //      specified value.\n        this.model = this.lstmLayerSize.reduce(\n            (mdl, lstmLayerSize, i, orig) => {\n                mdl.add(\n                    tf.layers.lstm({\n                        units: lstmLayerSize,\n                        // XXX: For all layers except the last one, we specify that we'll be returning\n                        //      sequences of data. This allows us to iteratively chain individual LSTMs\n                        //      to one-another.\n                        returnSequences: i < orig.length - 1,\n                        // XXX: Since each LSTM layer generates a sequence of data, only the first layer\n                        //      needs to receive a specific input shape. Here, we initialize the inputShape\n                        //      [sampleLen, this.characters.length]. This defines that the first layer will receive an\n                        //      input matrix which allows us to convert from our selected sample range into\n                        //      the size of our charset. The charset uses one-hot encoding, which allows us\n                        //      to represent each possible character in our dataset using a dedicated input\n                        //      neuron.\n                        inputShape:\n                            i === 0\n                                ? [this.sampleLen, this.characters.length]\n                                : undefined\n                    })\n                )\n                // XXX: Here we use a sequential processing model for our network. This model gets passed\n                //      between each iteration, and is what we add our LSTM layers to.\n                return mdl\n            },\n            tf.sequential()\n        )\n\n        // XXX: At the output, we use a softmax function (a normalized exponential) as the final\n        //      classification layer. This is common in many neural networks. It's particularly\n        //      important for this example, because we use the logit probability model (which\n        //      supports regression for networks with more than 2 possible outcomes of a categorically\n        //      distributed dependent variable).\n        this.model.add(\n            tf.layers.dense({\n                units: this.characters.length,\n                activation: 'softmax'\n            })\n        )\n\n        // XXX: Finally, compile the model. The optimizer is used to define the backpropagation\n        //      technique that should be used for training. We use the rmsProp to help tune the\n        //      learning rate that we apply individually to each neuron to help learning.\n        //      We use a categoricalCrossentropy loss model which is compatible with our softmax\n        //      activation output.\n        this.model.compile({\n            optimizer: tf.train.rmsprop(this.learningRate),\n            loss: 'categoricalCrossentropy'\n        })\n    }\n\n    getModel() {\n        return this.model\n    }\n\n    async trainModel(dataGenerator) {\n        const bound = trainModel.bind(this)\n        await bound(dataGenerator)\n    }\n\n    getWeights() {\n        return this.model.getWeights()\n    }\n\n    async generate(seed, temperature = 0.7) {\n        const bound = generate.bind(this)\n        return await bound(seed, temperature)\n    }\n}\n\nasync function generate(seed, temperature) {\n    // XXX: Fetch the sequence of numeric values which correspond to the\n    //      sentence.\n    let sentenceIndices = Array.from(seed).map((e) =>\n        this.characters.indexOf(e)\n    )\n\n    let generated = ''\n\n    // XXX: Note that since the displayLength is arbitrary, we can make it\n    //      much larger than our sampleLen. This loop will continue to iterate\n    //      about the sentenceIndices and buffering the output of the network,\n    //      which permits it to continue generating far past our initial seed\n    //      has been provided.\n    while (generated.length < this.displayLength) {\n        const inputBuffer = new tf.TensorBuffer([\n            1,\n            this.sampleLen,\n            this.characters.length\n        ])\n\n        ;[...Array(this.sampleLen)].map((_, i) =>\n            inputBuffer.set(1, 0, i, sentenceIndices[i])\n        )\n\n        const input = inputBuffer.toTensor()\n        const output = this.model.predict(input)\n\n        // XXX: Pick the character the RNN has decided is the most likely.\n        //      tf.tidy cleans all of the allocated tensors within the function\n        //      scope after it has been executed.\n        const [winnerIndex] = tf.tidy(() =>\n            // XXX: Draws samples from a multinomial distribution (these are distributions\n            //      involving multiple variables).\n            //      tf.squeeze remove dimensions of size (1) from the supplied tensor. These\n            //      are then divided by the specified temperature.\n            tf\n                .multinomial(\n                    // XXX: Use the temperature to control the network's spontaneity.\n                    tf.div(\n                        tf.log(tf.squeeze(output)),\n                        Math.max(temperature, 1e-6)\n                    ),\n                    1,\n                    null,\n                    false\n                )\n                .dataSync()\n        )\n\n        // XXX: Always clean up tensors once you're finished with them to improve\n        //      memory utilization and prevent leaks.\n        input.dispose()\n        output.dispose()\n\n        // XXX: Here we append the generated character to the resulting string, and\n        //      add this char to the sliding window along the sentenceIndices. This\n        //      is how we continually wrap around the same buffer and generate arbitrary\n        //      sequences of data even though our network only accepts fixed inputs.\n        generated += this.characters[winnerIndex]\n        sentenceIndices = sentenceIndices.slice(1)\n        sentenceIndices.push(winnerIndex)\n    }\n    // console.log(`Generated text (temperature=${temperature}):\\n ${generated}\\n`)\n    return generated\n}\n","import '@tensorflow/tfjs-node'\nimport * as tf from '@tensorflow/tfjs'\n\nexport async function trainModel(dataGenerator) {\n    // XXX: .\n    const sampleLen = 60 // length of a sequence of characters we'll pass into the RNN\n    const sampleStep = 3 // number of characters to jump between segments of input text\n    const epochs = 150 // the total number of times to update the training weights\n    const examplesPerEpoch = 10000 // the number of text segments to train against for each epoch\n    const batchSize = 128 // hyperparameter controlling the frequency weights are updated\n    const validationSplit = 0.0625 // fraction of training data which will be treated as validation data\n\n    const seed = ''\n    for (let i = 0; i < epochs; ++i) {\n        const ds = tf.data.generator(\n            createBatchGenerator(dataGenerator, self.characters)\n        )\n        await this.model.fitDataset(ds, {\n            epochs: 1,\n            batchSize,\n            // validationSplit,\n            callbacks: {\n                onTrainBegin: () => console.log(`Epoch ${i + 1} of ${epochs}:`),\n                onBatchEnd: async (batch, logs) => console.log(logs),\n                onEpochEnd: async (epoch, logs) =>\n                    console.log(await this.generate(seed, 0.7))\n            }\n        })\n    }\n}\n\nfunction createBatchGenerator(dataGenerator) {\n    return function* () {\n        yield* batchGenerator(dataGenerator, characters)\n    }\n}\n\nfunction* batchGenerator(dataGenerator, characters) {\n    console.log('trying to load batches')\n    while (true) {\n        const text = dataGenerator.next().value\n\n        console.log(text)\n\n        // Extract necessary parameters from text or context\n        const textLength = text.length\n        const sampleLen = 60 // Adjust as needed\n        const sampleStep = 3 // Adjust as needed\n        // const charSet = Array.from(new Set(Array.from(text)))\n        // const charSetSize = charSet.length\n\n        // Create tensors for the current batch\n        const textIndices = new Uint16Array(\n            Array.from(text).map((e) => characters.indexOf(e))\n        )\n        const trainingIndices = []\n\n        for (let i = 0; i < textLength - sampleLen - 1; i += sampleStep) {\n            trainingIndices.push(i)\n        }\n\n        tf.util.shuffle(trainingIndices)\n\n        const xsBuffer = new tf.TensorBuffer([1, sampleLen, characters.length]) // One example per batch\n        const ysBuffer = new tf.TensorBuffer([1, characters.length])\n\n        const batchIndex = trainingIndices[0 % trainingIndices.length]\n        for (let j = 0; j < sampleLen; ++j) {\n            xsBuffer.set(1, 0, j, textIndices[batchIndex + j])\n        }\n        ysBuffer.set(1, 0, textIndices[batchIndex + sampleLen])\n\n        yield { xs: xsBuffer.toTensor(), ys: ysBuffer.toTensor() }\n    }\n}\n"],"names":[],"version":3,"file":"index.js.map","sourceRoot":"../"}