{"mappings":";;;;AEEO,eAAe,0CAAW,aAAa,EAAE,YAAY,GAAG;IAC3D,MAAM,kBAAkB,OAAO,qEAAqE;;IAEpG,MAAM,OAAO;IAEb,MAAM,KAAK,YAAQ,SAAS,CACxB,2CAAqB,eAAe,IAAI,CAAC,KAAK;IAElD,MAAM,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,IAAI;QAC5B,QAAQ;QACR,aAAa;QACb,mBAAmB;QACnB,WAAW;YACP,cAAc,KAAO;YACrB,YAAY,OAAO,OAAO;gBACtB,QAAQ,GAAG,CAAC;gBACZ,IAAK,IAAI,QAAQ;oBAAC;oBAAG;oBAAK;oBAAK;oBAAK;iBAAI,CAAE;oBACtC,MAAM,SAAS,MAAM,IAAI,CAAC,QAAQ,CAAC,IAAI;oBACvC,QAAQ,GAAG,CAAC;gBAChB;YACJ;YACA,YAAY,OAAO,OAAO,OAAS,QAAQ,GAAG,CAAC;QACnD;IACJ;AACJ;AAEA,SAAS,2CAAqB,aAAa,EAAE,KAAK;IAC9C,OAAO;QACH,OAAO,qCAAe,eAAe;IACzC;AACJ;AAEA,UAAU,qCAAe,aAAa,EAAE,KAAK;IACzC,MAAO,KAAM;QACT,MAAM,QAAQ,EAAE;QAChB,MAAM,YAAY;QAClB,MAAM,kBAAkB;QACxB,IAAK,IAAI,IAAI,GAAG,IAAI,WAAW,EAAE,EAAG;YAChC,MAAM,OAAO,cAAc,IAAI,GAAG,KAAK;YAEvC,2CAA2C;YAC3C,MAAM,sBAAsB,KACvB,KAAK,CAAC,IACN,GAAG,CAAC,CAAC,OAAS,MAAM,OAAO,CAAC,OAC5B,MAAM,CAAC,CAAC,QAAU,UAAU,KAAK,yCAAyC;YAE/E,kCAAkC;YAClC,MAAM,gBAAgB,gBAAY,qBAAqB;YACvD,MAAM,sBAAsB,aAAS,eAAe;gBAAC;gBAAG,kBAAkB,oBAAoB,MAAM;aAAC;YAErG,4EAA4E;YAC5E,MAAM,gBAAgB,MAAM,oBAAoB,KAAK,IAAI,uEAAuE;YAChI,oBAAoB,OAAO,IAAI,gCAAgC;YAE/D,MAAM,WAAW,cAAU;gBAAC;gBAAiB,MAAM,MAAM;aAAC;YAC1D,MAAM,WAAW,cAAU;gBAAC,MAAM,MAAM;aAAC;YAEzC,IAAK,IAAI,IAAI,GAAG,IAAI,cAAc,MAAM,EAAE,EAAE,EACxC,SAAS,GAAG,CAAC,GAAG,GAAG,aAAa,CAAC,EAAE;YAGvC,2DAA2D;YAC3D,0EAA0E;YAC1E,MAAM,aAAa,MAAM,OAAO,CAAC,IAAI,CAAC,gBAAgB,GAAG,mCAAmC;YAC5F,IAAI,eAAe,IACf,SAAS,GAAG,CAAC,GAAG;YAGpB,MAAM,IAAI,CAAC;gBACP,IAAI,SAAS,QAAQ;gBACrB,IAAI,SAAS,QAAQ;YACzB;QACJ;QACA,MAAM;YACF,IAAI,aAAS,MAAM,GAAG,CAAC,CAAC,SAAW,OAAO,EAAE;YAC5C,IAAI,aAAS,MAAM,GAAG,CAAC,CAAC,SAAW,OAAO,EAAE;QAChD;IACJ;AACJ;;;AD7EA,QAAQ,GAAG,CAAC,YAAY;AAET;IACX,YAAY,aAAa,EAAE,SAAS,EAAE,YAAY,EAAE,aAAa,CAAE;QAC/D,IAAI,CAAC,aAAa,GAAG;QACrB,IAAI,CAAC,SAAS,GAAG;QACjB,IAAI,CAAC,KAAK,GAAG,MAAM,IAAI,CACnB,IAAI,IACA,MAAM,IAAI,CACN,CAAC;CAAkF,CAAC;QAIhG,IAAI,CAAC,YAAY,GAAG;QACpB,IAAI,CAAC,aAAa,GAAG;QACrB,IAAI,CAAC,KAAK,GAAG;QACb,IAAI,CAAC,IAAI;IACb;IAEA,OAAO;QACH,sFAAsF;QACtF,yFAAyF;QACzF,wBAAwB;QACxB,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,aAAa,CAAC,MAAM,CAClC,CAAC,KAAK,eAAe,GAAG;YACpB,IAAI,GAAG,CACH,cAAU,IAAI,CAAC;gBACX,OAAO;gBACP,8EAA8E;gBAC9E,+EAA+E;gBAC/E,uBAAuB;gBACvB,iBAAiB,IAAI,KAAK,MAAM,GAAG;gBACnC,gFAAgF;gBAChF,mFAAmF;gBACnF,yFAAyF;gBACzF,mFAAmF;gBACnF,mFAAmF;gBACnF,mFAAmF;gBACnF,eAAe;gBACf,YACI,MAAM,IACA;oBAAC,IAAI,CAAC,SAAS;oBAAE,IAAI,CAAC,KAAK,CAAC,MAAM;iBAAC,GACnC;YACd;YAEJ,yFAAyF;YACzF,sEAAsE;YACtE,OAAO;QACX,GACA;QAGJ,wFAAwF;QACxF,uFAAuF;QACvF,qFAAqF;QACrF,8FAA8F;QAC9F,wCAAwC;QACxC,IAAI,CAAC,KAAK,CAAC,GAAG,CACV,cAAU,KAAK,CAAC;YACZ,OAAO,IAAI,CAAC,KAAK,CAAC,MAAM;YACxB,YAAY;QAChB;QAGJ,uFAAuF;QACvF,uFAAuF;QACvF,iFAAiF;QACjF,wFAAwF;QACxF,0BAA0B;QAC1B,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC;YACf,WAAW,aAAS,OAAO,CAAC,IAAI,CAAC,YAAY;YAC7C,MAAM;QACV;IACJ;IAEA,WAAW;QACP,OAAO,IAAI,CAAC,KAAK;IACrB;IAEA,MAAM,WAAW,aAAa,EAAE;QAC5B,MAAM,QAAQ,CAAA,GAAA,yCAAS,EAAE,IAAI,CAAC,IAAI;QAClC,MAAM,MAAM;IAChB;IAEA,aAAa;QACT,OAAO,IAAI,CAAC,KAAK,CAAC,UAAU;IAChC;IAEA,MAAM,SAAS,IAAI,EAAE,cAAc,GAAG,EAAE;QACpC,MAAM,QAAQ,+BAAS,IAAI,CAAC,IAAI;QAChC,OAAO,MAAM,MAAM,MAAM;IAC7B;AACJ;AAEA,eAAe,+BAAS,IAAI,EAAE,WAAW;IACrC,oEAAoE;IACpE,iBAAiB;IACjB,IAAI,kBAAkB,MAAM,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,IAAM,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC;IAErE,IAAI,YAAY;IAEhB,sEAAsE;IACtE,0EAA0E;IAC1E,0EAA0E;IAC1E,yEAAyE;IACzE,0BAA0B;IAC1B,MAAO,UAAU,MAAM,GAAG,IAAI,CAAC,aAAa,CAAE;QAC1C,MAAM,cAAc,IAAI,oBAAgB;YACpC;YACA,IAAI,CAAC,SAAS;YACd,IAAI,CAAC,KAAK,CAAC,MAAM;SACpB;QAEA;eAAI,MAAM,IAAI,CAAC,SAAS;SAAE,CAAC,GAAG,CAAC,CAAC,GAAG,IAChC,YAAY,GAAG,CAAC,GAAG,GAAG,GAAG,eAAe,CAAC,EAAE;QAG/C,MAAM,QAAQ,YAAY,QAAQ;QAClC,MAAM,SAAS,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC;QAElC,kEAAkE;QAClE,uEAAuE;QACvE,yCAAyC;QACzC,MAAM,CAAC,YAAY,GAAG,YAAQ,IAC1B,8EAA8E;YAC9E,sCAAsC;YACtC,gFAAgF;YAChF,sDAAsD;YACtD,mBAEQ,iEAAiE;YACjE,WACI,WAAO,eAAW,UAClB,KAAK,GAAG,CAAC,aAAa,QAE1B,GACA,MACA,OAEH,QAAQ;QAGjB,yEAAyE;QACzE,6CAA6C;QAC7C,MAAM,OAAO;QACb,OAAO,OAAO;QAEd,2EAA2E;QAC3E,2EAA2E;QAC3E,gFAAgF;QAChF,4EAA4E;QAC5E,aAAa,IAAI,CAAC,KAAK,CAAC,YAAY;QACpC,kBAAkB,gBAAgB,KAAK,CAAC;QACxC,gBAAgB,IAAI,CAAC;IACzB;IACA,+EAA+E;IAC/E,OAAO;AACX;;;ID/JA,2CAAe,CAAA,GAAA,wCAAI","sources":["src/index.js","src/model.js","src/train.js"],"sourcesContent":["import Model from './model'\nexport default Model\n","import * as tf from '@tensorflow/tfjs-node'\nimport { trainModel } from './train'\n\nconsole.log('Backend:', tf.backend())\n\nexport default class ModelPrototype {\n    constructor(lstmLayerSize, sampleLen, learningRate, displayLength) {\n        this.lstmLayerSize = lstmLayerSize\n        this.sampleLen = sampleLen\n        this.vocab = Array.from(\n            new Set(\n                Array.from(\n                    `Â¶0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.?!'\"(){}[]|/\\\\\\n `\n                )\n            )\n        )\n        this.learningRate = learningRate\n        this.displayLength = displayLength\n        this.model = null\n        this.init()\n    }\n\n    init() {\n        // XXX: Create our processing model. We iterate through the array of lstmLayerSize and\n        //      iteratively add an LSTM processing layer whose number of internal units match the\n        //      specified value.\n        this.model = this.lstmLayerSize.reduce(\n            (mdl, lstmLayerSize, i, orig) => {\n                mdl.add(\n                    tf.layers.lstm({\n                        units: lstmLayerSize,\n                        // XXX: For all layers except the last one, we specify that we'll be returning\n                        //      sequences of data. This allows us to iteratively chain individual LSTMs\n                        //      to one-another.\n                        returnSequences: i < orig.length - 1,\n                        // XXX: Since each LSTM layer generates a sequence of data, only the first layer\n                        //      needs to receive a specific input shape. Here, we initialize the inputShape\n                        //      [sampleLen, this.vocab.length]. This defines that the first layer will receive an\n                        //      input matrix which allows us to convert from our selected sample range into\n                        //      the size of our charset. The charset uses one-hot encoding, which allows us\n                        //      to represent each possible character in our dataset using a dedicated input\n                        //      neuron.\n                        inputShape:\n                            i === 0\n                                ? [this.sampleLen, this.vocab.length]\n                                : undefined\n                    })\n                )\n                // XXX: Here we use a sequential processing model for our network. This model gets passed\n                //      between each iteration, and is what we add our LSTM layers to.\n                return mdl\n            },\n            tf.sequential()\n        )\n\n        // XXX: At the output, we use a softmax function (a normalized exponential) as the final\n        //      classification layer. This is common in many neural networks. It's particularly\n        //      important for this example, because we use the logit probability model (which\n        //      supports regression for networks with more than 2 possible outcomes of a categorically\n        //      distributed dependent variable).\n        this.model.add(\n            tf.layers.dense({\n                units: this.vocab.length,\n                activation: 'softmax'\n            })\n        )\n\n        // XXX: Finally, compile the model. The optimizer is used to define the backpropagation\n        //      technique that should be used for training. We use the rmsProp to help tune the\n        //      learning rate that we apply individually to each neuron to help learning.\n        //      We use a categoricalCrossentropy loss model which is compatible with our softmax\n        //      activation output.\n        this.model.compile({\n            optimizer: tf.train.rmsprop(this.learningRate),\n            loss: 'categoricalCrossentropy'\n        })\n    }\n\n    getModel() {\n        return this.model\n    }\n\n    async trainModel(dataGenerator) {\n        const bound = trainModel.bind(this)\n        await bound(dataGenerator)\n    }\n\n    getWeights() {\n        return this.model.getWeights()\n    }\n\n    async generate(seed, temperature = 0.7) {\n        const bound = generate.bind(this)\n        return await bound(seed, temperature)\n    }\n}\n\nasync function generate(seed, temperature) {\n    // XXX: Fetch the sequence of numeric values which correspond to the\n    //      sentence.\n    let sentenceIndices = Array.from(seed).map((e) => this.vocab.indexOf(e))\n\n    let generated = ''\n\n    // XXX: Note that since the displayLength is arbitrary, we can make it\n    //      much larger than our sampleLen. This loop will continue to iterate\n    //      about the sentenceIndices and buffering the output of the network,\n    //      which permits it to continue generating far past our initial seed\n    //      has been provided.\n    while (generated.length < this.displayLength) {\n        const inputBuffer = new tf.TensorBuffer([\n            1,\n            this.sampleLen,\n            this.vocab.length\n        ])\n\n        ;[...Array(this.sampleLen)].map((_, i) =>\n            inputBuffer.set(1, 0, i, sentenceIndices[i])\n        )\n\n        const input = inputBuffer.toTensor()\n        const output = this.model.predict(input)\n\n        // XXX: Pick the character the RNN has decided is the most likely.\n        //      tf.tidy cleans all of the allocated tensors within the function\n        //      scope after it has been executed.\n        const [winnerIndex] = tf.tidy(() =>\n            // XXX: Draws samples from a multinomial distribution (these are distributions\n            //      involving multiple variables).\n            //      tf.squeeze remove dimensions of size (1) from the supplied tensor. These\n            //      are then divided by the specified temperature.\n            tf\n                .multinomial(\n                    // XXX: Use the temperature to control the network's spontaneity.\n                    tf.div(\n                        tf.log(tf.squeeze(output)),\n                        Math.max(temperature, 1e-6)\n                    ),\n                    1,\n                    null,\n                    false\n                )\n                .dataSync()\n        )\n\n        // XXX: Always clean up tensors once you're finished with them to improve\n        //      memory utilization and prevent leaks.\n        input.dispose()\n        output.dispose()\n\n        // XXX: Here we append the generated character to the resulting string, and\n        //      add this char to the sliding window along the sentenceIndices. This\n        //      is how we continually wrap around the same buffer and generate arbitrary\n        //      sequences of data even though our network only accepts fixed inputs.\n        generated += this.vocab[winnerIndex]\n        sentenceIndices = sentenceIndices.slice(1)\n        sentenceIndices.push(winnerIndex)\n    }\n    // console.log(`Generated text (temperature=${temperature}):\\n ${generated}\\n`)\n    return generated\n}\n","import * as tf from '@tensorflow/tfjs-node'\n\nexport async function trainModel(dataGenerator, batchSize = 256) {\n    const validationSplit = 0.0625 // fraction of training data which will be treated as validation data\n\n    const seed = ''\n\n    const ds = tf.data.generator(\n        createBatchGenerator(dataGenerator, this.vocab)\n    )\n    await this.model.fitDataset(ds, {\n        epochs: 1,\n        // batchSize,\n        // validationSplit,\n        callbacks: {\n            onTrainBegin: () => {},\n            onBatchEnd: async (batch, logs) => {\n                console.log(logs)\n                for (let temp in [0, 0.3, 0.7, 0.9, 1.1]) {\n                    const output = await this.generate('', temp)\n                    console.log(output)\n                }\n            },\n            onEpochEnd: async (epoch, logs) => console.log('epoch ended')\n        }\n    })\n}\n\nfunction createBatchGenerator(dataGenerator, vocab) {\n    return function* () {\n        yield* batchGenerator(dataGenerator, vocab)\n    }\n}\n\nfunction* batchGenerator(dataGenerator, vocab) {\n    while (true) {\n        const batch = [];\n        const batchSize = 64;\n        const maxSampleLength = 180;\n        for (let i = 0; i < batchSize; ++i) {\n            const text = dataGenerator.next().value;\n\n            // Filter characters and convert to indices\n            const filteredTextIndices = text\n                .split('')\n                .map((char) => vocab.indexOf(char))\n                .filter((index) => index !== -1); // Ensure only valid indices are included\n\n            // Create a Tensor and then pad it\n            const indicesTensor = tf.tensor1d(filteredTextIndices, 'int32');\n            const paddedIndicesTensor = tf.pad1d(indicesTensor, [0, maxSampleLength - filteredTextIndices.length]);\n\n            // Convert the padded tensor back to an array for setting values in xsBuffer\n            const paddedIndices = await paddedIndicesTensor.array(); // Note: this requires the function to be async or handling the promise\n            paddedIndicesTensor.dispose(); // Dispose tensor to free memory\n\n            const xsBuffer = tf.buffer([maxSampleLength, vocab.length]);\n            const ysBuffer = tf.buffer([vocab.length]);\n\n            for (let j = 0; j < paddedIndices.length; ++j) {\n                xsBuffer.set(1, j, paddedIndices[j]);\n            }\n\n            // Assuming the label is the next character in the sequence\n            // This part needs to be adjusted based on your actual label setting logic\n            const labelIndex = vocab.indexOf(text[maxSampleLength]); // Get index of the label character\n            if (labelIndex !== -1) {\n                ysBuffer.set(1, labelIndex);\n            }\n\n            batch.push({\n                xs: xsBuffer.toTensor(),\n                ys: ysBuffer.toTensor()\n            });\n        }\n        yield {\n            xs: tf.stack(batch.map((sample) => sample.xs)),\n            ys: tf.stack(batch.map((sample) => sample.ys))\n        };\n    }\n}\n\n"],"names":[],"version":3,"file":"index.js.map","sourceRoot":"../"}