{"mappings":";;;ACEO,eAAe,0CACpB,aAAa,EACb,SAAS,EACT,WAAW,EACX,YAAY;IAEZ,sFAAsF;IACtF,yFAAyF;IACzF,wBAAwB;IACxB,MAAM,QAAQ,cAAc,MAAM,CAAC,CAAC,KAAK,eAAe,GAAG;QACzD,IAAI,GAAG,CACL,cAAU,IAAI,CAAC;YACb,OAAO;YACP,8EAA8E;YAC9E,+EAA+E;YAC/E,uBAAuB;YACvB,iBAAiB,IAAI,KAAK,MAAM,GAAG;YACnC,gFAAgF;YAChF,mFAAmF;YACnF,mFAAmF;YACnF,mFAAmF;YACnF,mFAAmF;YACnF,mFAAmF;YACnF,eAAe;YACf,YAAY,MAAM,IAAI;gBAAC;gBAAW;aAAY,GAAG;QACnD;QAEF,yFAAyF;QACzF,sEAAsE;QACtE,OAAO;IACT,GAAG;IAEH,wFAAwF;IACxF,uFAAuF;IACvF,qFAAqF;IACrF,8FAA8F;IAC9F,wCAAwC;IACxC,MAAM,GAAG,CACP,cAAU,KAAK,CAAC;QACd,OAAO;QACP,YAAY;IACd;IAGF,uFAAuF;IACvF,uFAAuF;IACvF,iFAAiF;IACjF,wFAAwF;IACxF,0BAA0B;IAC1B,MAAM,OAAO,CAAC;QACZ,WAAW,aAAS,OAAO,CAAC;QAC5B,MAAM;IACR;IAEA,OAAO;AACT;;;CDvDA,8BAA8B;CAC9B,oCAAoC;CAEpC,+BAA+B;CAC/B,yCAAyC;CAEzC,gGAAgG;CAChG,6FAA6F;CAC7F,+FAA+F;CAC/F,6FAA6F;CAC7F,+CAA+C;CAC/C,iEAAiE;CACjE,cAAc;CACd,gGAAgG;CAEhG,+FAA+F;CAC/F,oGAAoG;CACpG,6FAA6F;CAC7F,4FAA4F;CAC5F,gEAAgE;CAChE,iFAAiF;CACjF,2FAA2F;CAC3F,8FAA8F;CAC9F,6BAA6B;CAC7B,0EAA0E;CAC1E,eAAe;CACf,yBAAyB;CACzB,gCAAgC;CAChC,yFAAyF;CACzF,0FAA0F;CAC1F,kCAAkC;CAClC,gDAAgD;CAChD,2FAA2F;CAC3F,8FAA8F;CAC9F,8FAA8F;CAC9F,8FAA8F;CAC9F,8FAA8F;CAC9F,8FAA8F;CAC9F,0BAA0B;CAC1B,qEAAqE;CACrE,WAAW;CACX,QAAQ;CACR,gGAAgG;CAChG,6EAA6E;CAC7E,iBAAiB;CACjB,wBAAwB;CAExB,6FAA6F;CAC7F,4FAA4F;CAC5F,0FAA0F;CAC1F,mGAAmG;CACnG,6CAA6C;CAC7C,eAAe;CACf,wBAAwB;CACxB,4BAA4B;CAC5B,8BAA8B;CAC9B,SAAS;CACT,MAAM;CAEN,4FAA4F;CAC5F,4FAA4F;CAC5F,sFAAsF;CACtF,6FAA6F;CAC7F,+BAA+B;CAC/B,oBAAoB;CACpB,iDAAiD;CACjD,sCAAsC;CACtC,OAAO;CAEP,iBAAiB;CACjB,IAAI;CAEJ,8EAA8E;CAC9E,8EAA8E;CAC9E,kFAAkF;CAClF,+EAA+E;CAC/E,8EAA8E;CAC9E,8EAA8E;CAC9E,8EAA8E;CAC9E,yCAAyC;CACzC,0BAA0B;CAC1B,gBAAgB;CAChB,eAAe;CACf,gBAAgB;CAChB,iBAAiB;CACjB,iBAAiB;CACjB,gBAAgB;CAChB,SAAS;CACT,+BAA+B;CAE/B,uEAAuE;CACvE,8BAA8B;CAC9B,MAAM;CAEN,qCAAqC;CAErC,gFAAgF;CAEhF,qEAAqE;CAErE,4CAA4C;CAC5C,qEAAqE;CACrE,4CAA4C;CAC5C,2DAA2D;CAC3D,QAAQ;CACR,8DAA8D;CAC9D,MAAM;CAEN,sDAAsD;CACtD,IAAI;CAEJ,0EAA0E;CAC1E,4EAA4E;CAC5E,gFAAgF;CAChF,4EAA4E;CAC5E,0EAA0E;CAC1E,qBAAqB;CACrB,WAAW;CACX,UAAU;CACV,eAAe;CACf,iBAAiB;CACjB,aAAa;CACb,mBAAmB;CACnB,gBAAgB;CAChB,SAAS;CACT,yEAAyE;CACzE,sBAAsB;CACtB,0EAA0E;CAE1E,uBAAuB;CAEvB,2EAA2E;CAC3E,+EAA+E;CAC/E,+EAA+E;CAC/E,8EAA8E;CAC9E,+BAA+B;CAC/B,+CAA+C;CAC/C,2EAA2E;CAE3E,2CAA2C;CAC3C,qDAAqD;CACrD,QAAQ;CAER,2CAA2C;CAC3C,0CAA0C;CAE1C,yEAAyE;CACzE,8EAA8E;CAC9E,gDAAgD;CAChD,0CAA0C;CAC1C,uFAAuF;CACvF,+CAA+C;CAC/C,yFAAyF;CACzF,+DAA+D;CAC/D,WAAW;CACX,wBAAwB;CACxB,8EAA8E;CAC9E,6EAA6E;CAC7E,eAAe;CACf,kBAAkB;CAClB,kBAAkB;CAClB,YAAY;CACZ,sBAAsB;CACtB,QAAQ;CAER,gFAAgF;CAChF,oDAAoD;CACpD,sBAAsB;CACtB,uBAAuB;CAEvB,kFAAkF;CAClF,kFAAkF;CAClF,uFAAuF;CACvF,mFAAmF;CACnF,wCAAwC;CACxC,iDAAiD;CACjD,wCAAwC;CACxC,MAAM;CACN,iFAAiF;CACjF,IAAI;CAEJ,kBAAkB;CAClB,cAAc;CACd,uFAAuF;CACvF,wFAAwF;CACxF,2FAA2F;CAC3F,mFAAmF;CACnF,kGAAkG;CAClG,0FAA0F;CAC1F,yGAAyG;CACzG,yFAAyF;CACzF,yFAAyF;CACzF,iDAAiD;CAEjD,gDAAgD;CAChD,yCAAyC;CACzC,qBAAqB;CACrB,UAAU;CACV,OAAO;CAEP,mEAAmE;CACnE,0DAA0D;CAC1D,4CAA4C;CAE5C,mFAAmF;CAEnF,oBAAoB;CAEpB,yFAAyF;CACzF,wFAAwF;CACxF,iCAAiC;CACjC,yCAAyC;CACzC,sDAAsD;CACtD,MAAM;CAEN,0FAA0F;CAC1F,2EAA2E;CAC3E,iFAAiF;CAEjF,4EAA4E;CAC5E,gEAAgE;CAEhE,uCAAuC;CACvC,sCAAsC;CACtC,qBAAqB;CACrB,mBAAmB;CACnB,oBAAoB;CACpB,qBAAqB;CACrB,qBAAqB;CACrB,yBAAyB;CACzB,QAAQ;CAER,kEAAkE;CAClE,oCAAoC;CACpC,gCAAgC;CAChC,mBAAmB;CACnB,mBAAmB;CACnB,yBAAyB;CACzB,qBAAqB;CACrB,gCAAgC;CAChC,wDAAwD;CACxD,aAAa;CACb,4BAA4B;CAC5B,uCAAuC;CACvC,wBAAwB;CACxB,uBAAuB;CACvB,sBAAsB;CACtB,2BAA2B;CAC3B,6BAA6B;CAC7B,yBAAyB;CACzB,+BAA+B;CAC/B,qBAAqB;CACrB,gBAAgB;CAChB,cAAc;CACd,UAAU;CACV,SAAS;CAET,mBAAmB;CACnB,mBAAmB;CACnB,MAAM;CACN,OAAO","sources":["src/index.js","src/models.js"],"sourcesContent":["import { createModel } from './models'\n\n// // import '@babel/polyfill'\n// // import '@tensorflow/tfjs-node'\n\n// // import axios from 'axios'\n// import * as tf from '@tensorflow/tfjs'\n\n// // XXX: Define the url to pull text from. Here, we're using the tensorflowjs/tfjs Shakespeare\n// //      text corpus (giant blob). What's special about this data source is it shows that a\n// //      neural network can be trained to learn and emulate a specific style of writing. This\n// //      was first popularised by the now famous article \"The Unreasonable Effectiveness of\n// //      Neural Networks\" by Andrej Kaparthy:\n// //      http://karpathy.github.io/2015/05/21/rnn-effectiveness\n// const url =\n//   'https://storage.googleapis.com/tfjs-examples/lstm-text-generation/data/t8.shakespeare.txt'\n\n// // XXX: Creates a Long-Short-Term-Memory (LSTM) model. These are specific kinds of Recurrent\n// //      Neural Networks (RNNs) which have optimized neuron coefficients to improve the robustness\n// //      of backpropagation (training). RNNs are effectively chains of layers which process\n// //      individual time steps in a sequence of processing, and exploit the internal state\n// //      (memory) of each layer to achieve complex operations.\n// const createModel = (lstmLayerSize, sampleLen, charSetSize, learningRate) => {\n//   // XXX: Create our processing model. We iterate through the array of lstmLayerSize and\n//   //      iteratively add an LSTM processing layer whose number of internal units match the\n//   //      specified value.\n//   const model = lstmLayerSize.reduce((mdl, lstmLayerSize, i, orig) => {\n//     mdl.add(\n//       tf.layers.lstm({\n//         units: lstmLayerSize,\n//         // XXX: For all layers except the last one, we specify that we'll be returning\n//         //      sequences of data. This allows us to iteratively chain individual LSTMs\n//         //      to one-another.\n//         returnSequences: i < orig.length - 1,\n//         // XXX: Since each LSTM layer generates a sequence of data, only the first layer\n//         //      needs to receive a specific input shape. Here, we initialize the inputShape\n//         //      [sampleLen, charSetSize]. This defines that the first layer will receive an\n//         //      input matrix which allows us to convert from our selected sample range into\n//         //      the size of our charset. The charset uses one-hot encoding, which allows us\n//         //      to represent each possible character in our dataset using a dedicated input\n//         //      neuron.\n//         inputShape: i === 0 ? [sampleLen, charSetSize] : undefined\n//       })\n//     )\n//     // XXX: Here we use a sequential processing model for our network. This model gets passed\n//     //      between each iteration, and is what we add our LSTM layers to.\n//     return mdl\n//   }, tf.sequential())\n\n//   // XXX: At the output, we use a softmax function (a normalized exponential) as the final\n//   //      classification layer. This is common in many neural networks. It's particularly\n//   //      important for this example, because we use the logit probability model (which\n//   //      supports regression for networks with more than 2 possible outcomes of a categorically\n//   //      distributed dependent variable).\n//   model.add(\n//     tf.layers.dense({\n//       units: charSetSize,\n//       activation: 'softmax'\n//     })\n//   )\n\n//   // XXX: Finally, compile the model. The optimizer is used to define the backpropagation\n//   //      technique that should be used for training. We use the rmsProp to help tune the\n//   //      learning rate that we apply individually to each neuron to help learning.\n//   //      We use a categoricalCrossentropy loss model which is compatible with our softmax\n//   //      activation output.\n//   model.compile({\n//     optimizer: tf.train.rmsprop(learningRate),\n//     loss: 'categoricalCrossentropy'\n//   })\n\n//   return model\n// }\n\n// // XXX: This function separates the input data stream into segments of data\n// //      which can be used for a training epoch. We select segments from the\n// //      input which create the \"size\" of data we're interested in and use these\n// //      to index from the vectorized character set. These indices need to be\n// //      shuffled to prevent the RNN from accidentally learning the sequence\n// //      these segments usually arrive in. Finally, the mapped data elements\n// //      are packaged into tensors which can be used to drive the inputs and\n// //      outputs of the neural network.\n// const nextDataEpoch = (\n//   textLength,\n//   sampleLen,\n//   sampleStep,\n//   charSetSize,\n//   textIndices,\n//   numExamples\n// ) => {\n//   const trainingIndices = []\n\n//   for (let i = 0; i < textLength - sampleLen - 1; i += sampleStep) {\n//     trainingIndices.push(i)\n//   }\n\n//   tf.util.shuffle(trainingIndices)\n\n//   const xsBuffer = new tf.TensorBuffer([numExamples, sampleLen, charSetSize])\n\n//   const ysBuffer = new tf.TensorBuffer([numExamples, charSetSize])\n\n//   for (let i = 0; i < numExamples; ++i) {\n//     const beginIndex = trainingIndices[i % trainingIndices.length]\n//     for (let j = 0; j < sampleLen; ++j) {\n//       xsBuffer.set(1, i, j, textIndices[beginIndex + j])\n//     }\n//     ysBuffer.set(1, i, textIndices[beginIndex + sampleLen])\n//   }\n\n//   return [xsBuffer.toTensor(), ysBuffer.toTensor()]\n// }\n\n// // XXX: Takes the generated LSTM character prediction model and uses it\n// //      to predict character data. We use a seed string to initialize the\n// //      generation, which effectively kicks the neural network into producing\n// //      data. Once this window of data is finished, the neural network is\n// //      effectively seeding itself, and \"hallucinating\" its own output.\n// const generate = (\n//   model,\n//   seed,\n//   sampleLen,\n//   charSetSize,\n//   charSet,\n//   displayLength,\n//   temperature\n// ) => {\n//   // XXX: Fetch the sequence of numeric values which correspond to the\n//   //      sentence.\n//   let sentenceIndices = Array.from(seed).map((e) => charSet.indexOf(e))\n\n//   let generated = ''\n\n//   // XXX: Note that since the displayLength is arbitrary, we can make it\n//   //      much larger than our sampleLen. This loop will continue to iterate\n//   //      about the sentenceIndices and buffering the output of the network,\n//   //      which permits it to continue generating far past our initial seed\n//   //      has been provided.\n//   while (generated.length < displayLength) {\n//     const inputBuffer = new tf.TensorBuffer([1, sampleLen, charSetSize])\n\n//     ;[...Array(sampleLen)].map((_, i) =>\n//       inputBuffer.set(1, 0, i, sentenceIndices[i])\n//     )\n\n//     const input = inputBuffer.toTensor()\n//     const output = model.predict(input)\n\n//     // XXX: Pick the character the RNN has decided is the most likely.\n//     //      tf.tidy cleans all of the allocated tensors within the function\n//     //      scope after it has been executed.\n//     const [winnerIndex] = tf.tidy(() =>\n//       // XXX: Draws samples from a multinomial distribution (these are distributions\n//       //      involving multiple variables).\n//       //      tf.squeeze remove dimensions of size (1) from the supplied tensor. These\n//       //      are then divided by the specified temperature.\n//       tf\n//         .multinomial(\n//           // XXX: Use the temperature to control the network's spontaneity.\n//           tf.div(tf.log(tf.squeeze(output)), Math.max(temperature, 1e-6)),\n//           1,\n//           null,\n//           false\n//         )\n//         .dataSync()\n//     )\n\n//     // XXX: Always clean up tensors once you're finished with them to improve\n//     //      memory utilization and prevent leaks.\n//     input.dispose()\n//     output.dispose()\n\n//     // XXX: Here we append the generated character to the resulting string, and\n//     //      add this char to the sliding window along the sentenceIndices. This\n//     //      is how we continually wrap around the same buffer and generate arbitrary\n//     //      sequences of data even though our network only accepts fixed inputs.\n//     generated += charSet[winnerIndex]\n//     sentenceIndices = sentenceIndices.slice(1)\n//     sentenceIndices.push(winnerIndex)\n//   }\n//   console.log(`Generated text (temperature=${temperature}):\\n ${generated}\\n`)\n// }\n\n// ;(async () => {\n//   // XXX: .\n//   const sampleLen = 60 // length of a sequence of characters we'll pass into the RNN\n//   const sampleStep = 3 // number of characters to jump between segments of input text\n//   const learningRate = 1e-2 // higher values lead to faster convergence, but more errors\n//   const epochs = 150 // the total number of times to update the training weights\n//   const examplesPerEpoch = 10000 // the number of text segments to train against for each epoch\n//   const batchSize = 128 // hyperparameter controlling the frequency weights are updated\n//   const validationSplit = 0.0625 // fraction of training data which will be treated as validation data\n//   const displayLength = 120 // how many characters you want to generate after training\n//   const lstmLayerSize = [128, 128] // the configuration of eah sequential lstm network\n//   const temperatures = [0, 0.25, 0.5, 0.75, 1]\n\n//   // XXX: Fetch the text data to sample from.\n//   const { data: text } = await axios({\n//     method: 'get',\n//     url\n//   })\n\n//   // XXX: Fetch all unique characters in the dataset. (quickly!)\n//   const charSet = Array.from(new Set(Array.from(text)))\n//   const { length: charSetSize } = charSet\n\n//   const model = createModel(lstmLayerSize, sampleLen, charSetSize, learningRate)\n\n//   model.summary()\n\n//   // XXX: Convert the total input character text into the corresponding indices in the\n//   //      charSet. This is how we map consistently between character data and numeric\n//   //      neural network dataj\n//   const textIndices = new Uint16Array(\n//     Array.from(text).map((e) => charSet.indexOf(e))\n//   )\n\n//   // XXX: Pick a random position to start in the dataset. (Note that we choose an index\n//   //      which cannot exceed the minimum size of our sampleLength - 1).\n//   const startIndex = Math.round(Math.random() * (text.length - sampleLen - 1))\n\n//   // XXX: Create the seed data which we'll use to initialize the network.\n//   const seed = text.slice(startIndex, startIndex + sampleLen)\n\n//   for (let i = 0; i < epochs; ++i) {\n//     const [xs, ys] = nextDataEpoch(\n//       text.length,\n//       sampleLen,\n//       sampleStep,\n//       charSetSize,\n//       textIndices,\n//       examplesPerEpoch\n//     )\n\n//     // XXX: Fit the model and hold up iteration of the for loop\n//     //      until it is finished.\n//     await model.fit(xs, ys, {\n//       epochs: 1,\n//       batchSize,\n//       validationSplit,\n//       callbacks: {\n//         onTrainBegin: () => {\n//           console.log(`Epoch ${i + 1} of ${epochs}:`)\n//         },\n//         onTrainEnd: () =>\n//           temperatures.map((temp) =>\n//             generate(\n//               model,\n//               seed,\n//               sampleLen,\n//               charSetSize,\n//               charSet,\n//               displayLength,\n//               temp\n//             )\n//           )\n//       }\n//     })\n\n//     xs.dispose()\n//     ys.dispose()\n//   }\n// })()\n","import * as tf from '@tensorflow/tfjs'\n\nexport async function createModel(\n  lstmLayerSize,\n  sampleLen,\n  charSetSize,\n  learningRate\n) {\n  // XXX: Create our processing model. We iterate through the array of lstmLayerSize and\n  //      iteratively add an LSTM processing layer whose number of internal units match the\n  //      specified value.\n  const model = lstmLayerSize.reduce((mdl, lstmLayerSize, i, orig) => {\n    mdl.add(\n      tf.layers.lstm({\n        units: lstmLayerSize,\n        // XXX: For all layers except the last one, we specify that we'll be returning\n        //      sequences of data. This allows us to iteratively chain individual LSTMs\n        //      to one-another.\n        returnSequences: i < orig.length - 1,\n        // XXX: Since each LSTM layer generates a sequence of data, only the first layer\n        //      needs to receive a specific input shape. Here, we initialize the inputShape\n        //      [sampleLen, charSetSize]. This defines that the first layer will receive an\n        //      input matrix which allows us to convert from our selected sample range into\n        //      the size of our charset. The charset uses one-hot encoding, which allows us\n        //      to represent each possible character in our dataset using a dedicated input\n        //      neuron.\n        inputShape: i === 0 ? [sampleLen, charSetSize] : undefined\n      })\n    )\n    // XXX: Here we use a sequential processing model for our network. This model gets passed\n    //      between each iteration, and is what we add our LSTM layers to.\n    return mdl\n  }, tf.sequential())\n\n  // XXX: At the output, we use a softmax function (a normalized exponential) as the final\n  //      classification layer. This is common in many neural networks. It's particularly\n  //      important for this example, because we use the logit probability model (which\n  //      supports regression for networks with more than 2 possible outcomes of a categorically\n  //      distributed dependent variable).\n  model.add(\n    tf.layers.dense({\n      units: charSetSize,\n      activation: 'softmax'\n    })\n  )\n\n  // XXX: Finally, compile the model. The optimizer is used to define the backpropagation\n  //      technique that should be used for training. We use the rmsProp to help tune the\n  //      learning rate that we apply individually to each neuron to help learning.\n  //      We use a categoricalCrossentropy loss model which is compatible with our softmax\n  //      activation output.\n  model.compile({\n    optimizer: tf.train.rmsprop(learningRate),\n    loss: 'categoricalCrossentropy'\n  })\n\n  return model\n}\n"],"names":[],"version":3,"file":"module.js.map","sourceRoot":"../"}