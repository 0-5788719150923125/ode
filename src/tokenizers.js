import TokenMonsterCore from './tokenizers/tokenmonster.cjs'
import { env } from '@xenova/transformers'
env.allowLocalModels = typeof window !== 'undefined' ? true : false
import { AutoTokenizer } from '@xenova/transformers'

class TokenizerBase {
    constructor() {
        // pass
    }

    getLength() {
        return this.vocab.length
    }

    getConfig() {
        return {
            class: this.constructor.name
        }
    }

    encode(string) {
        // not implemented
    }

    decode(array) {
        // not implemented
    }

    writeVocabularyToFile(path) {
        // skip
    }
}

class CharacterTokenizer extends TokenizerBase {
    constructor(config) {
        super(config)
        const vocab =
            config?.vocab ||
            `\n \r\n \r \u2028\u2029!"#$%&'()*+,-.\\/0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_\`abcdefghijklmnopqrstuvwxyz{|}~Â¡Â£Â§Â©Â¬Â®Â°Â±Â²Â³ÂµÂ¶Â·Â¹ÂºÂ»Â¼Â½Ã€ÃÃ„Ã…Ã‡Ã‰ÃŽÃ“Ã”Ã–Ã—Ã˜ÃœÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã­Ã®Ã¯Ã°Ã±Ã³Ã´ÃµÃ¶Ã·Ã¸ÃºÃ¼Ä€ÄÄƒÄ‡ÄŒÄÄ‘Ä“Ä›ÄŸÄ§Ä«Ä°Ä±Å‚Å„ÅÅ‘Å›ÅŸÅ¡Å«Å¯Å·ÅºÅ½Å¾È§È²È³É›ÊŒÊ»Ê¼Ë†ËšÌ‚Ì„Ì…Ì¸Î‘Î’Î“Î”Î˜Î›Î Î£Î¦Î¨Î©Î­Î®Î¯Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¿Ï€ÏÏ‚ÏƒÏ„Ï†Ï‡ÏˆÏ‰ÏŒÏµÐºÑ°á¸—á¹“á»£á¼´á½â€‹â€â€“â€”â€˜â€™â€œâ€â€ â€¢â€¤â€¦â€¯â€²â€³â´â»â‚€â‚â‚‚â‚ƒâ‚„â‚–â‚™â‚¬â„“â„•â„šâ„â„¢â„¤â…“â†â†‘â†’â†“â†”â†¦â‡Œâ‡’â‡”â‡ â‡¥âˆ€âˆ‚âˆƒâˆ…âˆ†âˆ‡âˆˆâˆ‰âˆâˆ‘âˆ’âˆ—âˆ˜âˆ™âˆšâˆžâˆ âˆ§âˆ¨âˆ©âˆªâˆ«âˆ¼â‰…â‰ˆâ‰ â‰¡â‰¤â‰¥â‰«âŠ‚âŠ†âŠ•âŠ™â‹…âŒˆâŒ‰âŒ˜â”€â”‚â””â”œâ– â–¡â–¶â—†â—‹â—â—¦â™¢â™¥â™¦âœ“â¤âŸ¨âŸ©â¨¯â©½â±¼â²œã€‚ãƒ»ä¸–å‰åŠ¡å‘åŽå‘Šå‘¨åœ¨å°†æˆ‘æŠ¥æ–°æ›´æœ«æœ¬ç•Œçš„ç»™è¯·è´¢é€é“\ud835\ud835ð…\udf48\udf73`
        this.padToken = 'ï¿½'
        this.tokens = Array.from(new Set(vocab))
        this.tokens.unshift(this.padToken)
        console.log('Parsed vocabulary:')
        console.log(JSON.stringify(this.tokens.sort()))
    }

    getLength() {
        return this.tokens.length
    }

    getConfig() {
        return {
            ...super.getConfig(),
            length: this.getLength()
        }
    }

    encode(string) {
        return Array.from(string).map((char) => {
            const index = this.tokens.indexOf(char)
            return index !== -1 ? index : this.tokens.indexOf(this.padToken)
        })
    }

    decode(array) {
        return array
            .map((index) => {
                return index >= 0 && index < this.tokens.length
                    ? this.tokens[index]
                    : this.padToken
            })
            .join('')
    }
}

class XenovaTokenizer extends TokenizerBase {
    constructor(config) {
        super(config)
        this.model = config.model || 'openai-community/gpt2'
        this.tokenizer
    }

    async init() {
        this.tokenizer = await AutoTokenizer.from_pretrained(this.model)
    }

    getLength() {
        return this.tokenizer.model.vocab.length
    }

    getConfig() {
        return {
            ...super.getConfig(),
            model: this.model,
            length: this.getLength()
        }
    }

    encode(string) {
        return this.tokenizer.encode(string)
    }

    decode(array) {
        return this.tokenizer.decode(array, { skip_special_tokens: true })
    }
}

// https://github.com/alasdairforsythe/tokenmonster
class TokenMonster extends TokenizerBase {
    constructor(config) {
        super(config)
        this.model = config.model || 'englishcode-32000-consistent-v1'
    }

    async init() {
        this.tokenizer = new TokenMonsterCore()
        await this.tokenizer.load(
            `https://huggingface.co/alasdairforsythe/tokenmonster/raw/main/vocabs/${this.model}.vocab`
        )
        this.decoder = this.tokenizer.Decoder()
    }

    getLength() {
        return this.tokenizer.vocab_size
    }

    getConfig() {
        return {
            ...super.getConfig(),
            model: this.model,
            length: this.getLength()
        }
    }

    encode(string) {
        return this.tokenizer.tokenize(string)
    }

    decode(array) {
        return this.decoder.detokenize(array)
    }
}

const tokenizers = {
    CharacterTokenizer: (config) => new CharacterTokenizer(config),
    XenovaTokenizer: (config) => new XenovaTokenizer(config),
    TokenMonster: (config) => new TokenMonster(config)
}
export default tokenizers
